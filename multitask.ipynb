{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multitask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/easyri/paintings/blob/master/multitask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA_MBd7s6k5s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIJ08RFl-XC3"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# ! pip install git+https://github.com/keras-team/autokeras.git@master\n",
        "# ! pip install git+https://github.com/keras-team/keras-tuner.git@master\n",
        "# ! pip install tensorflow==2.2.0\n",
        "import os, glob, cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import resnet50\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions, ResNet50\n",
        "from keras.layers import Flatten, Input\n",
        "import keras \n",
        "from keras import layers\n",
        "import torch\n",
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "import argparse\n",
        "import os\n",
        "from torchvision import transforms, utils, datasets, models\n",
        "from torch.utils.data import DataLoader, ConcatDataset, Dataset\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import glob\n",
        "import copy\n",
        "import shutil\n",
        "import sys\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Reshape, Concatenate\n",
        "from collections import defaultdict\n",
        "import string\n",
        "from os import listdir\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from pickle import dump\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pickle import load, dump\n",
        "import argparse\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.layers import Input, Reshape, Concatenate\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM, GRU\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "lstm_layers = 1\n",
        "dropout_rate = 0.5\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO1O5Vri0m8P"
      },
      "source": [
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "    # skip empty lines\n",
        "    if len(line) < 1:\n",
        "      continue\n",
        "    # get the image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "\n",
        "\n",
        "def load_photo_arrays(filename, dataset):\n",
        "  # load all features\n",
        "  all = load(open(filename, 'rb'))\n",
        "  # filter features\n",
        "  features = {k: all[k] for k in dataset}\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvCP72t9vvSf"
      },
      "source": [
        "# calculate the length of the description with the most words\n",
        "def max_len(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  return max(len(d.split()) for d in lines)\n",
        "\n",
        "\n",
        "  # convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "  \n",
        "def load_clean_descriptions(filename, dataset):\n",
        "  # load document\n",
        "  doc = load_doc(filename)\n",
        "  descriptions = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "    # split line by white space\n",
        "    tokens = line.split()\n",
        "    # split id from description\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    # skip images not in the set\n",
        "    if image_id in dataset:\n",
        "      # create list\n",
        "      if image_id not in descriptions:\n",
        "        descriptions[image_id] = list()\n",
        "      # wrap description in tokens\n",
        "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "      # store\n",
        "      descriptions[image_id].append(desc)\n",
        "  return descriptions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQqJ7KFkG1gY",
        "outputId": "0f283538-9106-4b01-df03-f820ae6e041d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d950wydoKLN8"
      },
      "source": [
        "flikr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA25KlGOsJUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a942bd-954e-4671-eed7-f09b8c3c876e"
      },
      "source": [
        "\n",
        "ftrain_names = '/content/drive/My Drive/paintings/Flickr_8k.trainImages.txt'\n",
        "ftest_names = '/content/drive/My Drive/paintings/Flickr_8k.devImages.txt'\n",
        "ftrain = load_set(ftrain_names)\n",
        "ftest = load_set(ftest_names)\n",
        "ftrain_features = load_photo_arrays('/content/drive/My Drive/paintings/flikr_features.pkl', ftrain)\n",
        "ftest_features = load_photo_arrays('/content/drive/My Drive/paintings/flikr_features.pkl', ftest)\n",
        "print(len(ftrain_features.keys()))\n",
        "print(len(ftest_features.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n",
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJytAHOAvDL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da9b508f-07bd-408d-d9bf-aad74b79a74e"
      },
      "source": [
        "ftokenizer = load(open('/content/drive/My Drive/paintings/flikr_tokenizer.pkl', 'rb'))\n",
        "findex_word = load(open('/content/drive/My Drive/paintings/flikr_index_word.pkl', 'rb'))\n",
        "fvocab_size = len(ftokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % fvocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 7579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWVwCj-8v-oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f706edc-1da8-40e6-e281-15e477e790f1"
      },
      "source": [
        "ftrain_descriptions = load_clean_descriptions('/content/drive/My Drive/paintings/flikr_descr.txt', ftrain)\n",
        "ftest_descriptions = load_clean_descriptions('/content/drive/My Drive/paintings/flikr_descr.txt', ftest)\n",
        "fmax_length = max_len(ftrain_descriptions)\n",
        "print('Description Length: %d' % fmax_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Z4dyzhKNzk"
      },
      "source": [
        "wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxLU0_SZKdcg"
      },
      "source": [
        "wfeatures = load(open('/content/drive/My Drive/paintings/wikiart/real_features.pkl', 'rb'))\r\n",
        "wnames = list(wfeatures.keys())\r\n",
        "wtrain, wtest = train_test_split(wnames, random_state=0)\r\n",
        "# print(wtrain, wtest, sep=' ')\r\n",
        "wtrain_features = {k: wfeatures[k] for k in wtrain}\r\n",
        "wtest_features = {k: wfeatures[k] for k in wtest}\r\n",
        "wtrain_descriptions = load_clean_descriptions('/content/drive/MyDrive/paintings/real_descr.txt', wtrain)\r\n",
        "# print(train_descriptions)\r\n",
        "wtest_descriptions = load_clean_descriptions('/content/drive/MyDrive/paintings/real_descr.txt', wtest)\r\n",
        "wmax_length = max_len(wtrain_descriptions)\r\n",
        "# print(max_length)\r\n",
        "wtokenizer = create_tokenizer(wtrain_descriptions)\r\n",
        "# dump(tokenizer, open('/content/drive/My Drive/paintings/real_tokenizer.pkl', 'wb'))\r\n",
        "windex_word = {v: k for k, v in wtokenizer.word_index.items()}\r\n",
        "# dump(index_word, open('/content/drive/My Drive/paintings/real_index_word.pkl', 'wb'))\r\n",
        "\r\n",
        "wvocab_size = len(wtokenizer.word_index) + 1\r\n",
        "print('Vocabulary Size: %d' % wvocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrRCtn4sqgg2",
        "outputId": "f79db7e9-7ccb-4d12-b22e-b68a4ea7372c"
      },
      "source": [
        "new_wtrain, new_wtest = wtrain[:6000], wtest[:1000]\r\n",
        "wtrain_features = {k: wfeatures[k] for k in new_wtrain}\r\n",
        "wtest_features = {k: wfeatures[k] for k in new_wtest}\r\n",
        "wtrain_descriptions = load_clean_descriptions('/content/drive/MyDrive/paintings/real_descr.txt', new_wtrain)\r\n",
        "# print(train_descriptions)\r\n",
        "wtest_descriptions = load_clean_descriptions('/content/drive/MyDrive/paintings/real_descr.txt', new_wtest)\r\n",
        "wmax_length = max_len(wtrain_descriptions)\r\n",
        "# print(max_length)\r\n",
        "wtokenizer = create_tokenizer(wtrain_descriptions)\r\n",
        "# dump(tokenizer, open('/content/drive/My Drive/paintings/real_tokenizer.pkl', 'wb'))\r\n",
        "windex_word = {v: k for k, v in wtokenizer.word_index.items()}\r\n",
        "# dump(index_word, open('/content/drive/My Drive/paintings/real_index_word.pkl', 'wb'))\r\n",
        "\r\n",
        "wvocab_size = len(wtokenizer.word_index) + 1\r\n",
        "print('Vocabulary Size: %d' % wvocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 5582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZP5M-P_tOwS"
      },
      "source": [
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  X1, X2, y = [], [], []\n",
        "  # walk through each description for the image\n",
        "  for desc in desc_list:\n",
        "    # encode the sequence\n",
        "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "    # split one sequence into multiple X,y pairs\n",
        "    for i in range(1, len(seq)):\n",
        "      # split into input and output pair\n",
        "      in_seq, out_seq = seq[:i], seq[i]\n",
        "      # pad input sequence\n",
        "      in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "      # encode output sequence\n",
        "      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "      # store\n",
        "      X1.append(photo)\n",
        "      X2.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "  return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, n_step = 1):\n",
        "  # loop for ever over images\n",
        "  while True:\n",
        "    # loop over photo identifiers in the dataset\n",
        "    keys = list(descriptions.keys())\n",
        "    for i in range(0, len(keys), n_step):\n",
        "      Ximages, XSeq, y = list(), list(),list()\n",
        "      for j in range(i, min(len(keys), i+n_step)):\n",
        "        image_id = keys[j]\n",
        "        # retrieve the photo feature\n",
        "        photo = photos[image_id][0]\n",
        "        desc_list = descriptions[image_id]\n",
        "        # print(desc_list)\n",
        "        in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
        "        # print(in_img, in_seq, out_word)\n",
        "        for k in range(len(in_img)):\n",
        "          Ximages.append(in_img[k])\n",
        "          XSeq.append(in_seq[k])\n",
        "          y.append(out_word[k])\n",
        "        \n",
        "      yield ([np.array(Ximages), np.array(XSeq)], np.array(y))\n",
        "\n",
        "\n",
        "def double_generator(descriptions1, photos1, tokenizer1, max_length1,\n",
        "                       descriptions2, photos2, tokenizer2, max_length2, n_step=1):\n",
        "  while True:\n",
        "    # loop over photo identifiers in the dataset\n",
        "    keys1 = list(descriptions1.keys())\n",
        "    keys2 = list(descriptions2.keys())    # len(keys1) == len(keys2)\n",
        "    for i in range(0, len(keys1), n_step):\n",
        "      Ximages1, XSeq1, y1 = list(), list(),list()\n",
        "      Ximages2, XSeq2, y2 = list(), list(),list()\n",
        "      for j in range(i, min(len(keys1), i+n_step)):\n",
        "        image_id1 = keys1[j]\n",
        "        # retrieve the photo feature\n",
        "        photo1 = photos1[image_id1][0]\n",
        "        desc_list1 = descriptions1[image_id1]\n",
        "        # print(desc_list)\n",
        "        in_img1, in_seq1, out_word1 = create_sequences(tokenizer1, max_length1, desc_list1, photo1)\n",
        "        # print(in_img, in_seq, out_word)\n",
        "        for k in range(len(in_img1)):\n",
        "          Ximages1.append(in_img1[k])\n",
        "          XSeq1.append(in_seq1[k])\n",
        "          y1.append(out_word1[k])\n",
        "        # print('Ximages1', Ximages1)\n",
        "        # print('Xseq1', XSeq1)\n",
        "        # print('y1', y1)\n",
        "      for j in range(i, min(len(keys2), i+n_step)):\n",
        "        image_id2 = keys2[j]\n",
        "        # retrieve the photo feature\n",
        "        photo2 = photos2[image_id2][0]\n",
        "        desc_list2 = descriptions2[image_id2]\n",
        "        # print(desc_list)\n",
        "        in_img2, in_seq2, out_word2 = create_sequences(tokenizer2, max_length2, desc_list2, photo2)\n",
        "        # print(in_img, in_seq, out_word)\n",
        "        for k in range(len(in_img2)):\n",
        "          Ximages2.append(in_img2[k])\n",
        "          XSeq2.append(in_seq2[k])\n",
        "          y2.append(out_word2[k])\n",
        "        # print('Ximages2', Ximages2)\n",
        "        # print('Xseq2', XSeq2)\n",
        "        # print('y2', y2)\n",
        "      yield ([np.array(Ximages1), np.array(XSeq1), np.array(Ximages2), np.array(XSeq2)], [np.array(y1), np.array(y2)])\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size1, max_length1, vocab_size2, max_length2):\n",
        "\n",
        "  # flikr\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  print(inputs1.shape)\n",
        "  fe1_1 = Dropout(0.5)(inputs1)\n",
        "  fe2_1 = Dense(EMBEDDING_DIM, activation='relu')(fe1_1)\n",
        "  fe3_1 = RepeatVector(max_length1)(fe2_1)\n",
        "\n",
        "  inputs2 = Input(shape=(max_length1,))\n",
        "  print(inputs2.shape)\n",
        "  emb2_1 = Embedding(vocab_size1, EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
        "  \n",
        "  merged1 = concatenate([fe3_1, emb2_1], name='concat1')\n",
        "  lm2_1 = LSTM(500, return_sequences=False)(merged1)\n",
        "  # print(lm2_1.shape)\n",
        "  # lm2_1 = Flatten()(lm2_1)\n",
        "\n",
        "\n",
        "  # wiki\n",
        "  inputs3 = Input(shape=(4096,))\n",
        "  fe1_2 = Dropout(0.5)(inputs3)\n",
        "  fe2_2 = Dense(EMBEDDING_DIM, activation='relu')(fe1_2)\n",
        "  fe3_2 = RepeatVector(max_length2)(fe2_2)\n",
        "  \n",
        "  inputs4 = Input(shape=(max_length2,))\n",
        "  emb2_2 = Embedding(vocab_size2, EMBEDDING_DIM, mask_zero=True)(inputs4)\n",
        "  \n",
        "  merged2 = concatenate([fe3_2, emb2_2], name='concat2')\n",
        "  lm2_2 = LSTM(500, return_sequences=False)(merged2)\n",
        "  # print(lm2_2.shape)\n",
        "  # lm2_2 = Flatten()(lm2_2)\n",
        "\n",
        "# merge\n",
        "  merged3 = concatenate([lm2_1, lm2_2], axis=0, name='concat3')\n",
        "  outputs = Dense(vocab_size1, activation='softmax')(merged3)\n",
        "  outputs1 = Dense(vocab_size2, activation='softmax')(merged3)\n",
        "\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3, inputs4], outputs=[outputs, outputs1])\n",
        "  model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy'], optimizer='adam', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  # plot_model(model, show_shapes=True, to_file='model.png')\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVVcMYGfZNP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da731fa7-828d-4df9-83a8-4d317b6bdf08"
      },
      "source": [
        "\r\n",
        "model = define_model(fvocab_size, fmax_length, wvocab_size, wmax_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 4096)\n",
            "(None, 34)\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_23 (InputLayer)           [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 4096)         0           input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 4096)         0           input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, 256)          1048832     dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "input_22 (InputLayer)           [(None, 34)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 256)          1048832     dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "input_24 (InputLayer)           [(None, 21)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_10 (RepeatVector) (None, 34, 256)      0           dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 34, 256)      1940224     input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_11 (RepeatVector) (None, 21, 256)      0           dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 21, 256)      1428992     input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concat1 (Concatenate)           (None, 34, 512)      0           repeat_vector_10[0][0]           \n",
            "                                                                 embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concat2 (Concatenate)           (None, 21, 512)      0           repeat_vector_11[0][0]           \n",
            "                                                                 embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 500)          2026000     concat1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_11 (LSTM)                  (None, 500)          2026000     concat2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concat3 (Concatenate)           (None, 500)          0           lstm_10[0][0]                    \n",
            "                                                                 lstm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, 5582)         2796582     concat3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, 7579)         3797079     concat3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 16,112,541\n",
            "Trainable params: 16,112,541\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoGSslMUyRjf"
      },
      "source": [
        "train_generator1 = data_generator(ftrain_descriptions, ftrain_features, ftokenizer, fmax_length)\r\n",
        "val_generator1 = data_generator(ftest_descriptions, ftest_features, ftokenizer, fmax_length)\r\n",
        "\r\n",
        "train_generator2 = data_generator(wtrain_descriptions, wtrain_features, wtokenizer, wmax_length)\r\n",
        "val_generator2 = data_generator(wtest_descriptions, wtest_features, wtokenizer, wmax_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS6vMPpVqqZw"
      },
      "source": [
        "train_generator = double_generator(ftrain_descriptions, ftrain_features, ftokenizer, fmax_length,\r\n",
        "                                   wtrain_descriptions, wtrain_features, wtokenizer, wmax_length)\r\n",
        "val_generator = double_generator(ftest_descriptions, ftest_features, ftokenizer, fmax_length,\r\n",
        "                                 wtest_descriptions, wtest_features, wtokenizer, wmax_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "ZK-H7RlLuGl1",
        "outputId": "3270444c-862c-4023-e92a-cf7e1d64d303"
      },
      "source": [
        "  # define checkpoint callback\r\n",
        "filepath = '/content/drive/My Drive/paintings/multi/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,save_freq='epoch',\r\n",
        "              save_best_only=False, save_weights_only=False, mode='min')\r\n",
        "\r\n",
        "steps = len(ftrain_descriptions)\r\n",
        "val_steps = len(ftest_descriptions)\r\n",
        "  # create the data generator\r\n",
        "\r\n",
        "\r\n",
        "  # fit model\r\n",
        " \r\n",
        "model.fit(train_generator, epochs=20,  verbose=1, steps_per_epoch=steps, validation_steps=val_steps,\r\n",
        "      callbacks=[checkpoint], validation_data=val_generator)\r\n",
        "\r\n",
        "try:\r\n",
        "    model.save('/content/drive/My Drive/paintings/multi/multi_wholeModel.h5', overwrite=True)\r\n",
        "    model.save_weights('/content/drive/My Drive/paintings/multi/multi_weights.h5',overwrite=True)\r\n",
        "except:\r\n",
        "    print(\"Error in saving model.\")\r\n",
        "print(\"Training complete...\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e52b85d1307b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m model.fit(train_generator, epochs=20,  verbose=1, steps_per_epoch=steps, validation_steps=val_steps,\n\u001b[0;32m---> 14\u001b[0;31m     callbacks=[checkpoint], validation_data=val_generator)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  logits and labels must be broadcastable: logits_size=[51,7579] labels_size=[4,5582]\n\t [[node categorical_crossentropy_1/softmax_cross_entropy_with_logits (defined at <ipython-input-28-e52b85d1307b>:14) ]] [Op:__inference_train_function_77835]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    }
  ]
}